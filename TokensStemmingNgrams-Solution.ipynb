{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all oriented left-to-right, so let's make a regex to find them. \n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emoji2 = re.compile(r\"^[:;=]-?.$\")\n",
    "emojis = {w for w in chat if emoji2.search(w)}\n",
    "sorted(emojis)\n",
    "#len(emojis)\n",
    "# could normalize by removing hyphens, case letters to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = text4[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vowels = re.compile(r'[aeiouyAEIOU]')\n",
    "\n",
    "len({w for w in nltk.corpus.words.words() if not vowels.search(w[:-3]) and w[-3:] == \"ing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.freq('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([w.lower() for w in text2 \n",
    "               if w.lower() not in \n",
    "               nltk.corpus.stopwords.words(\"english\") \n",
    "               and w.isalpha()])\n",
    "\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))\n",
    "    \n",
    "#4063/3861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram, count in fd.items() :\n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" \".join(gram) + \": \" + str(count))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram, count in sorted(fd.items(), key ) :\n",
    "    if gram[0] == \"I\" : \n",
    "        print(gram)\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need this for phrases\n",
    "from nltk.app import concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"sure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(freq_dist):\n",
    "    weight_total = sum([count for token,count in freq_dist.items()])\n",
    "    n = random.uniform(0, weight_total)\n",
    "    for token, count in freq_dist.items() :\n",
    "        if n < count:\n",
    "            return(token)\n",
    "        n = n - count\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_choice_ngram(cur_word,freq_dist) :\n",
    "    ''' Starts with a current word and randomly chooses \n",
    "        a following word based on the bigrams. '''\n",
    "    \n",
    "    # First, build list of tuples of the form\n",
    "    # ('a_word',count)\n",
    "    # where our freq_dist has an entry like \n",
    "    # ('cur_word','a_word',count)\n",
    "    sub_dist = {}\n",
    "    \n",
    "    for bigram, count in freq_dist.items() :\n",
    "        if bigram[0] == cur_word :\n",
    "            sub_dist[bigram[1]] = count\n",
    "    \n",
    "    return(weighted_choice(sub_dist))\n",
    "\n",
    "def generate_bigram(text,length=10,start=None) :\n",
    "    \n",
    "    if not start :\n",
    "        uni_fd = FreqDist(text)\n",
    "        start = weighted_choice(uni_fd)\n",
    "        \n",
    "    fd = FreqDist(nltk.bigrams(text))\n",
    "    \n",
    "    results = []\n",
    "    this_word = start\n",
    "    for i in range(length) :\n",
    "        this_word = weighted_choice_ngram(this_word,fd)\n",
    "        results.append(this_word)\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
