{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope and Functions\n",
    "\n",
    "Take a look at the code below. Guess what values are printed. If it's not obvious to you, we need to talk briefly about scoping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_fcn(val) :\n",
    "    x = 20\n",
    "    val = val + x\n",
    "    return(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "print(my_fcn(15))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Sandbox\n",
    "\n",
    "Throughout the class I'll ask for exercises. This notebook holds my exploration of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!=',\n",
       " '.:',\n",
       " '.;)',\n",
       " '//www.wunderground.com/cgi-bin/findweather/getForecast?query=95953#FIR',\n",
       " '10:49',\n",
       " '2:55',\n",
       " '3:45',\n",
       " '4:03',\n",
       " '6:38',\n",
       " '6:41',\n",
       " '6:51',\n",
       " '6:53',\n",
       " '7:45',\n",
       " '9:10',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':):):)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':beer:',\n",
       " ':blush:',\n",
       " ':love:',\n",
       " ':o *',\n",
       " ':p',\n",
       " ':tongue:',\n",
       " ':|',\n",
       " ';',\n",
       " '; ..',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=',\n",
       " \"='s\",\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p',\n",
       " '>:->',\n",
       " ']:)',\n",
       " 'capab;e',\n",
       " 'd=',\n",
       " 'http://forums.talkcity.com/tc-adults/start ',\n",
       " 'http://www.shadowbots.com',\n",
       " 'n;t',\n",
       " 'o<|=D'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(',\n",
       " ':)',\n",
       " ':-(',\n",
       " ':-)',\n",
       " ':-@',\n",
       " ':-o',\n",
       " ':.',\n",
       " ':/',\n",
       " ':@',\n",
       " ':D',\n",
       " ':O',\n",
       " ':P',\n",
       " ':]',\n",
       " ':p',\n",
       " ':|',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';0',\n",
       " ';]',\n",
       " ';p',\n",
       " '=(',\n",
       " '=)',\n",
       " '=-\\\\',\n",
       " '=/',\n",
       " '=D',\n",
       " '=O',\n",
       " '=[',\n",
       " '=]',\n",
       " '=p']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all oriented left-to-right, so let's make a regex to find them. \n",
    "emoji = re.compile(r\"^[:;=]-?[)(\\]PD@op|O]$\") # misses '>:->' and ']:)' and repeats. Insert shruggie\n",
    "emoji2 = re.compile(r\"^[:;=]-?.$\")\n",
    "emojis = {w for w in chat if emoji2.search(w)}\n",
    "sorted(emojis)\n",
    "#len(emojis)\n",
    "# could normalize by removing hyphens, case letters to upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"abcdefg\"\n",
    "\n",
    "x[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = text4[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels = re.compile(r'[aeiouyAEIOU]')\n",
    "\n",
    "len({w for w in nltk.corpus.words.words() if not vowels.search(w[:-3]) and w[-3:] == \"ing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid of that Almighty Power which has hitherto protected me and enabled me to bring to favorable issues other important but still greatly inferior trusts heretofore confided to me by my country . The broad foundation upon which our Constitution rests being the people -- a breath of theirs having made , as a breath can unmake , change , or modify it -- it can be assigned to none of the great divisions of government but to that of democracy . If such is its theory , those who are called upon to administer it must recognize as its\n",
      "\n",
      "\n",
      "\n",
      "aid of that Almighti Power which ha hitherto protect me and enabl me to bring to favor issu other import but still greatli inferior trust heretofor confid to me by my countri . The broad foundat upon which our Constitut rest be the peopl -- a breath of their have made , as a breath can unmak , chang , or modifi it -- it can be assign to none of the great divis of govern but to that of democraci . If such is it theori , those who are call upon to administ it must recogn as it\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5470\n",
      "1.783180987202925\n"
     ]
    }
   ],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in S&S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01443041193422614"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.freq('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elinor : 685 : 0.012688474789760307\n",
      "could : 578 : 0.010706479457637166\n",
      "marianne : 566 : 0.010484199607305598\n",
      "mrs : 530 : 0.009817360056310896\n",
      "would : 515 : 0.009539510243396436\n",
      "said : 397 : 0.0073537583818026895\n",
      "every : 377 : 0.0069832919645834105\n",
      "one : 331 : 0.006131219204979069\n",
      "much : 290 : 0.005371763049679547\n",
      "must : 283 : 0.005242099803652799\n",
      "sister : 282 : 0.005223576482791835\n",
      "edward : 263 : 0.00487163338643352\n",
      "mother : 258 : 0.0047790167821287\n",
      "dashwood : 252 : 0.004667876856962916\n",
      "well : 240 : 0.004445597006631349\n",
      "time : 239 : 0.004427073685770385\n",
      "know : 232 : 0.004297410439743637\n",
      "jennings : 230 : 0.004260363798021709\n",
      "though : 216 : 0.004001037305968214\n",
      "willoughby : 216 : 0.004001037305968214\n"
     ]
    }
   ],
   "source": [
    "fd = FreqDist([w.lower() for w in text2 if w.lower() not in nltk.corpus.stopwords.words(\"english\") and w.isalpha()])\n",
    "total_words = sum([count for word, count in fd.items()])\n",
    "\n",
    "for pairs in fd.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))\n",
    "    \n",
    "#4063/3861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist([\" \".join(b) for b in nltk.ngrams(text2,3) if b[0] == \"I\" and b[1] == \"am\"]) # could use bigram function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I am sure', 72),\n",
       " ('I am not', 12),\n",
       " ('I am sorry', 11),\n",
       " ('I am so', 11),\n",
       " ('I am afraid', 11),\n",
       " ('I am very', 10),\n",
       " ('I am now', 4),\n",
       " ('I am glad', 4),\n",
       " ('I am monstrous', 4),\n",
       " ('I am always', 3)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I turn: 1\n",
      "I walked: 2\n",
      "I fancy: 3\n",
      "I known: 1\n",
      "I happened: 3\n",
      "I suspect: 1\n",
      "I travelled: 1\n",
      "I admired: 1\n",
      "I profess: 1\n",
      "I told: 5\n",
      "I mentioned: 1\n",
      "I expected: 1\n",
      "I abhor: 1\n",
      "I take: 1\n",
      "I longed: 1\n",
      "I DO: 1\n",
      "I sha: 1\n",
      "I write: 1\n",
      "I fear: 5\n",
      "I more: 1\n",
      "I pity: 1\n",
      "I stop: 1\n",
      "I wonder: 11\n",
      "I only: 7\n",
      "I know: 56\n",
      "I assure: 17\n",
      "I heartily: 1\n",
      "I give: 1\n",
      "I directly: 1\n",
      "I ever: 13\n",
      "I both: 1\n",
      "I imagine: 1\n",
      "I trusted: 1\n",
      "I did: 25\n",
      "I tell: 9\n",
      "I talked: 3\n",
      "I call: 2\n",
      "I question: 1\n",
      "I alluded: 1\n",
      "I would: 35\n",
      "I got: 2\n",
      "I sent: 3\n",
      "I dare: 36\n",
      "I see: 13\n",
      "I wanted: 3\n",
      "I acquit: 1\n",
      "I ought: 3\n",
      "I endured: 1\n",
      "I to: 5\n",
      "I allowed: 1\n",
      "I earnestly: 1\n",
      "I ask: 3\n",
      "I speak: 2\n",
      "I remain: 1\n",
      "I avoided: 1\n",
      "I returned: 2\n",
      "I insist: 1\n",
      "I guessed: 2\n",
      "I felt: 18\n",
      "I might: 12\n",
      "I stay: 1\n",
      "I suffered: 2\n",
      "I learnt: 2\n",
      "I quitted: 1\n",
      "I value: 2\n",
      "I formed: 1\n",
      "I copied: 1\n",
      "I dreaded: 1\n",
      "I admire: 2\n",
      "I once: 2\n",
      "I may: 16\n",
      "I spoken: 1\n",
      "I not: 1\n",
      "I distress: 1\n",
      "I can: 56\n",
      "I immediately: 1\n",
      "I tried: 1\n",
      "I protest: 1\n",
      "I say: 3\n",
      "I find: 1\n",
      "I entreat: 2\n",
      "I sat: 1\n",
      "I don: 6\n",
      "I feared: 1\n",
      "I made: 3\n",
      "I feel: 6\n",
      "I express: 1\n",
      "I clearly: 1\n",
      "I need: 2\n",
      "I venture: 1\n",
      "I first: 2\n",
      "I trust: 1\n",
      "I compare: 2\n",
      "I took: 2\n",
      "I begged: 1\n",
      "I said: 6\n",
      "I fell: 1\n",
      "I look: 1\n",
      "I professed: 1\n",
      "I had: 66\n",
      "I no: 1\n",
      "I understood: 2\n",
      "I flatter: 1\n",
      "I saw: 14\n",
      "I acknowledge: 1\n",
      "I owed: 3\n",
      "I observed: 1\n",
      "I conceal: 1\n",
      "I left: 5\n",
      "I offered: 1\n",
      "I really: 4\n",
      "I hear: 4\n",
      "I cannot: 40\n",
      "I gave: 2\n",
      "I make: 2\n",
      "I could: 56\n",
      "I want: 1\n",
      "I contradicted: 1\n",
      "I advised: 1\n",
      "I undervalued: 1\n",
      "I TRIED: 1\n",
      "I .: 1\n",
      "I demand: 1\n",
      "I AM: 1\n",
      "I removed: 2\n",
      "I supply: 1\n",
      "I remained: 1\n",
      "I less: 1\n",
      "I --: 2\n",
      "I WILL: 4\n",
      "I still: 1\n",
      "I blundered: 1\n",
      "I neither: 1\n",
      "I went: 4\n",
      "I was: 86\n",
      "I just: 2\n",
      "I am: 223\n",
      "I scorn: 1\n",
      "I bring: 1\n",
      "I forget: 2\n",
      "I declare: 10\n",
      "I have: 192\n",
      "I thank: 2\n",
      "I DID: 3\n",
      "I endeavoured: 1\n",
      "I understand: 11\n",
      "I doubt: 1\n",
      "I suffer: 2\n",
      "I wish: 24\n",
      "I consider: 3\n",
      "I discharged: 1\n",
      "I now: 4\n",
      "I imitate: 1\n",
      "I go: 2\n",
      "I cut: 1\n",
      "I owe: 3\n",
      "I obey: 1\n",
      "I greatly: 1\n",
      "I particularly: 1\n",
      "I ': 6\n",
      "I mean: 8\n",
      "I will: 33\n",
      "I meet: 1\n",
      "I heard: 9\n",
      "I COULD: 2\n",
      "I confess: 9\n",
      "I wrote: 1\n",
      "I advise: 3\n",
      "I SHOULD: 1\n",
      "I reconciled: 1\n",
      "I a: 1\n",
      "I require: 2\n",
      "I began: 1\n",
      "I ran: 2\n",
      "I found: 5\n",
      "I shall: 64\n",
      "I detest: 2\n",
      "I reflect: 1\n",
      "I who: 2\n",
      "I approached: 1\n",
      "I knew: 7\n",
      "I think: 55\n",
      "I won: 1\n",
      "I .--: 1\n",
      "I hope: 42\n",
      "I reserved: 1\n",
      "I ,\": 1\n",
      "I used: 2\n",
      "I rather: 2\n",
      "I love: 7\n",
      "I leave: 1\n",
      "I loved: 3\n",
      "I set: 1\n",
      "I ,: 13\n",
      "I WAS: 1\n",
      "I rich: 1\n",
      "I shan: 2\n",
      "I meant: 3\n",
      "I quite: 1\n",
      "I looked: 6\n",
      "I hardly: 6\n",
      "I even: 2\n",
      "I guess: 2\n",
      "I certainly: 7\n",
      "I KNEW: 1\n",
      "I beg: 5\n",
      "I been: 3\n",
      "I came: 5\n",
      "I long: 1\n",
      "I spent: 3\n",
      "I chose: 1\n",
      "I care: 2\n",
      "I convinced: 1\n",
      "I called: 1\n",
      "I received: 1\n",
      "I well: 3\n",
      "I stayed: 1\n",
      "I considered: 1\n",
      "I come: 4\n",
      "I visited: 1\n",
      "I remember: 8\n",
      "I seem: 1\n",
      "I watched: 2\n",
      "I explained: 1\n",
      "I next: 1\n",
      "I cease: 1\n",
      "I often: 3\n",
      "I like: 5\n",
      "I wished: 2\n",
      "I must: 34\n",
      "I delighted: 1\n",
      "I do: 68\n",
      "I warrant: 3\n",
      "I dared: 1\n",
      "I never: 35\n",
      "I resolved: 1\n",
      "I met: 2\n",
      "I should: 69\n",
      "I presume: 1\n",
      "I then: 3\n",
      "I sincerely: 1\n",
      "I always: 7\n",
      "I persuaded: 1\n",
      "I wear: 1\n",
      "I meditated: 1\n",
      "I formerly: 1\n",
      "I were: 2\n",
      "I believe: 47\n",
      "I thought: 29\n",
      "I died: 2\n",
      "I relate: 1\n",
      "I an: 1\n",
      "I suppose: 32\n"
     ]
    }
   ],
   "source": [
    "for gram, count in fd.items() :\n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" \".join(gram) + \": \" + str(count))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = sum([count for pair, count in fd.items() if pair[0] == \"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2004"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', \"'\")\n",
      "6\n",
      "('I', ',')\n",
      "13\n",
      "('I', ',\"')\n",
      "1\n",
      "('I', '--')\n",
      "2\n",
      "('I', '.')\n",
      "1\n",
      "('I', '.--')\n",
      "1\n",
      "('I', 'AM')\n",
      "1\n",
      "('I', 'COULD')\n",
      "2\n",
      "('I', 'DID')\n",
      "3\n",
      "('I', 'DO')\n",
      "1\n",
      "('I', 'KNEW')\n",
      "1\n",
      "('I', 'SHOULD')\n",
      "1\n",
      "('I', 'TRIED')\n",
      "1\n",
      "('I', 'WAS')\n",
      "1\n",
      "('I', 'WILL')\n",
      "4\n",
      "('I', 'a')\n",
      "1\n",
      "('I', 'abhor')\n",
      "1\n",
      "('I', 'acknowledge')\n",
      "1\n",
      "('I', 'acquit')\n",
      "1\n",
      "('I', 'admire')\n",
      "2\n",
      "('I', 'admired')\n",
      "1\n",
      "('I', 'advise')\n",
      "3\n",
      "('I', 'advised')\n",
      "1\n",
      "('I', 'allowed')\n",
      "1\n",
      "('I', 'alluded')\n",
      "1\n",
      "('I', 'always')\n",
      "7\n",
      "('I', 'am')\n",
      "223\n",
      "('I', 'an')\n",
      "1\n",
      "('I', 'approached')\n",
      "1\n",
      "('I', 'ask')\n",
      "3\n",
      "('I', 'assure')\n",
      "17\n",
      "('I', 'avoided')\n",
      "1\n",
      "('I', 'been')\n",
      "3\n",
      "('I', 'beg')\n",
      "5\n",
      "('I', 'began')\n",
      "1\n",
      "('I', 'begged')\n",
      "1\n",
      "('I', 'believe')\n",
      "47\n",
      "('I', 'blundered')\n",
      "1\n",
      "('I', 'both')\n",
      "1\n",
      "('I', 'bring')\n",
      "1\n",
      "('I', 'call')\n",
      "2\n",
      "('I', 'called')\n",
      "1\n",
      "('I', 'came')\n",
      "5\n",
      "('I', 'can')\n",
      "56\n",
      "('I', 'cannot')\n",
      "40\n",
      "('I', 'care')\n",
      "2\n",
      "('I', 'cease')\n",
      "1\n",
      "('I', 'certainly')\n",
      "7\n",
      "('I', 'chose')\n",
      "1\n",
      "('I', 'clearly')\n",
      "1\n",
      "('I', 'come')\n",
      "4\n",
      "('I', 'compare')\n",
      "2\n",
      "('I', 'conceal')\n",
      "1\n",
      "('I', 'confess')\n",
      "9\n",
      "('I', 'consider')\n",
      "3\n",
      "('I', 'considered')\n",
      "1\n",
      "('I', 'contradicted')\n",
      "1\n",
      "('I', 'convinced')\n",
      "1\n",
      "('I', 'copied')\n",
      "1\n",
      "('I', 'could')\n",
      "56\n",
      "('I', 'cut')\n",
      "1\n",
      "('I', 'dare')\n",
      "36\n",
      "('I', 'dared')\n",
      "1\n",
      "('I', 'declare')\n",
      "10\n",
      "('I', 'delighted')\n",
      "1\n",
      "('I', 'demand')\n",
      "1\n",
      "('I', 'detest')\n",
      "2\n",
      "('I', 'did')\n",
      "25\n",
      "('I', 'died')\n",
      "2\n",
      "('I', 'directly')\n",
      "1\n",
      "('I', 'discharged')\n",
      "1\n",
      "('I', 'distress')\n",
      "1\n",
      "('I', 'do')\n",
      "68\n",
      "('I', 'don')\n",
      "6\n",
      "('I', 'doubt')\n",
      "1\n",
      "('I', 'dreaded')\n",
      "1\n",
      "('I', 'earnestly')\n",
      "1\n",
      "('I', 'endeavoured')\n",
      "1\n",
      "('I', 'endured')\n",
      "1\n",
      "('I', 'entreat')\n",
      "2\n",
      "('I', 'even')\n",
      "2\n",
      "('I', 'ever')\n",
      "13\n",
      "('I', 'expected')\n",
      "1\n",
      "('I', 'explained')\n",
      "1\n",
      "('I', 'express')\n",
      "1\n",
      "('I', 'fancy')\n",
      "3\n",
      "('I', 'fear')\n",
      "5\n",
      "('I', 'feared')\n",
      "1\n",
      "('I', 'feel')\n",
      "6\n",
      "('I', 'fell')\n",
      "1\n",
      "('I', 'felt')\n",
      "18\n",
      "('I', 'find')\n",
      "1\n",
      "('I', 'first')\n",
      "2\n",
      "('I', 'flatter')\n",
      "1\n",
      "('I', 'forget')\n",
      "2\n",
      "('I', 'formed')\n",
      "1\n",
      "('I', 'formerly')\n",
      "1\n",
      "('I', 'found')\n",
      "5\n",
      "('I', 'gave')\n",
      "2\n",
      "('I', 'give')\n",
      "1\n",
      "('I', 'go')\n",
      "2\n",
      "('I', 'got')\n",
      "2\n",
      "('I', 'greatly')\n",
      "1\n",
      "('I', 'guess')\n",
      "2\n",
      "('I', 'guessed')\n",
      "2\n",
      "('I', 'had')\n",
      "66\n",
      "('I', 'happened')\n",
      "3\n",
      "('I', 'hardly')\n",
      "6\n",
      "('I', 'have')\n",
      "192\n",
      "('I', 'hear')\n",
      "4\n",
      "('I', 'heard')\n",
      "9\n",
      "('I', 'heartily')\n",
      "1\n",
      "('I', 'hope')\n",
      "42\n",
      "('I', 'imagine')\n",
      "1\n",
      "('I', 'imitate')\n",
      "1\n",
      "('I', 'immediately')\n",
      "1\n",
      "('I', 'insist')\n",
      "1\n",
      "('I', 'just')\n",
      "2\n",
      "('I', 'knew')\n",
      "7\n",
      "('I', 'know')\n",
      "56\n",
      "('I', 'known')\n",
      "1\n",
      "('I', 'learnt')\n",
      "2\n",
      "('I', 'leave')\n",
      "1\n",
      "('I', 'left')\n",
      "5\n",
      "('I', 'less')\n",
      "1\n",
      "('I', 'like')\n",
      "5\n",
      "('I', 'long')\n",
      "1\n",
      "('I', 'longed')\n",
      "1\n",
      "('I', 'look')\n",
      "1\n",
      "('I', 'looked')\n",
      "6\n",
      "('I', 'love')\n",
      "7\n",
      "('I', 'loved')\n",
      "3\n",
      "('I', 'made')\n",
      "3\n",
      "('I', 'make')\n",
      "2\n",
      "('I', 'may')\n",
      "16\n",
      "('I', 'mean')\n",
      "8\n",
      "('I', 'meant')\n",
      "3\n",
      "('I', 'meditated')\n",
      "1\n",
      "('I', 'meet')\n",
      "1\n",
      "('I', 'mentioned')\n",
      "1\n",
      "('I', 'met')\n",
      "2\n",
      "('I', 'might')\n",
      "12\n",
      "('I', 'more')\n",
      "1\n",
      "('I', 'must')\n",
      "34\n",
      "('I', 'need')\n",
      "2\n",
      "('I', 'neither')\n",
      "1\n",
      "('I', 'never')\n",
      "35\n",
      "('I', 'next')\n",
      "1\n",
      "('I', 'no')\n",
      "1\n",
      "('I', 'not')\n",
      "1\n",
      "('I', 'now')\n",
      "4\n",
      "('I', 'obey')\n",
      "1\n",
      "('I', 'observed')\n",
      "1\n",
      "('I', 'offered')\n",
      "1\n",
      "('I', 'often')\n",
      "3\n",
      "('I', 'once')\n",
      "2\n",
      "('I', 'only')\n",
      "7\n",
      "('I', 'ought')\n",
      "3\n",
      "('I', 'owe')\n",
      "3\n",
      "('I', 'owed')\n",
      "3\n",
      "('I', 'particularly')\n",
      "1\n",
      "('I', 'persuaded')\n",
      "1\n",
      "('I', 'pity')\n",
      "1\n",
      "('I', 'presume')\n",
      "1\n",
      "('I', 'profess')\n",
      "1\n",
      "('I', 'professed')\n",
      "1\n",
      "('I', 'protest')\n",
      "1\n",
      "('I', 'question')\n",
      "1\n",
      "('I', 'quite')\n",
      "1\n",
      "('I', 'quitted')\n",
      "1\n",
      "('I', 'ran')\n",
      "2\n",
      "('I', 'rather')\n",
      "2\n",
      "('I', 'really')\n",
      "4\n",
      "('I', 'received')\n",
      "1\n",
      "('I', 'reconciled')\n",
      "1\n",
      "('I', 'reflect')\n",
      "1\n",
      "('I', 'relate')\n",
      "1\n",
      "('I', 'remain')\n",
      "1\n",
      "('I', 'remained')\n",
      "1\n",
      "('I', 'remember')\n",
      "8\n",
      "('I', 'removed')\n",
      "2\n",
      "('I', 'require')\n",
      "2\n",
      "('I', 'reserved')\n",
      "1\n",
      "('I', 'resolved')\n",
      "1\n",
      "('I', 'returned')\n",
      "2\n",
      "('I', 'rich')\n",
      "1\n",
      "('I', 'said')\n",
      "6\n",
      "('I', 'sat')\n",
      "1\n",
      "('I', 'saw')\n",
      "14\n",
      "('I', 'say')\n",
      "3\n",
      "('I', 'scorn')\n",
      "1\n",
      "('I', 'see')\n",
      "13\n",
      "('I', 'seem')\n",
      "1\n",
      "('I', 'sent')\n",
      "3\n",
      "('I', 'set')\n",
      "1\n",
      "('I', 'sha')\n",
      "1\n",
      "('I', 'shall')\n",
      "64\n",
      "('I', 'shan')\n",
      "2\n",
      "('I', 'should')\n",
      "69\n",
      "('I', 'sincerely')\n",
      "1\n",
      "('I', 'speak')\n",
      "2\n",
      "('I', 'spent')\n",
      "3\n",
      "('I', 'spoken')\n",
      "1\n",
      "('I', 'stay')\n",
      "1\n",
      "('I', 'stayed')\n",
      "1\n",
      "('I', 'still')\n",
      "1\n",
      "('I', 'stop')\n",
      "1\n",
      "('I', 'suffer')\n",
      "2\n",
      "('I', 'suffered')\n",
      "2\n",
      "('I', 'supply')\n",
      "1\n",
      "('I', 'suppose')\n",
      "32\n",
      "('I', 'suspect')\n",
      "1\n",
      "('I', 'take')\n",
      "1\n",
      "('I', 'talked')\n",
      "3\n",
      "('I', 'tell')\n",
      "9\n",
      "('I', 'thank')\n",
      "2\n",
      "('I', 'then')\n",
      "3\n",
      "('I', 'think')\n",
      "55\n",
      "('I', 'thought')\n",
      "29\n",
      "('I', 'to')\n",
      "5\n",
      "('I', 'told')\n",
      "5\n",
      "('I', 'took')\n",
      "2\n",
      "('I', 'travelled')\n",
      "1\n",
      "('I', 'tried')\n",
      "1\n",
      "('I', 'trust')\n",
      "1\n",
      "('I', 'trusted')\n",
      "1\n",
      "('I', 'turn')\n",
      "1\n",
      "('I', 'understand')\n",
      "11\n",
      "('I', 'understood')\n",
      "2\n",
      "('I', 'undervalued')\n",
      "1\n",
      "('I', 'used')\n",
      "2\n",
      "('I', 'value')\n",
      "2\n",
      "('I', 'venture')\n",
      "1\n",
      "('I', 'visited')\n",
      "1\n",
      "('I', 'walked')\n",
      "2\n",
      "('I', 'want')\n",
      "1\n",
      "('I', 'wanted')\n",
      "3\n",
      "('I', 'warrant')\n",
      "3\n",
      "('I', 'was')\n",
      "86\n",
      "('I', 'watched')\n",
      "2\n",
      "('I', 'wear')\n",
      "1\n",
      "('I', 'well')\n",
      "3\n",
      "('I', 'went')\n",
      "4\n",
      "('I', 'were')\n",
      "2\n",
      "('I', 'who')\n",
      "2\n",
      "('I', 'will')\n",
      "33\n",
      "('I', 'wish')\n",
      "24\n",
      "('I', 'wished')\n",
      "2\n",
      "('I', 'won')\n",
      "1\n",
      "('I', 'wonder')\n",
      "11\n",
      "('I', 'would')\n",
      "35\n",
      "('I', 'write')\n",
      "1\n",
      "('I', 'wrote')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for gram, count in sorted(fd.items(), key ) :\n",
    "    if gram[0] == \"I\" : \n",
    "        print(gram)\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'am') : 223 : 0.111\n",
      "('I', 'have') : 192 : 0.096\n",
      "('I', 'was') : 86 : 0.043\n",
      "('I', 'should') : 69 : 0.034\n",
      "('I', 'do') : 68 : 0.034\n",
      "('I', 'had') : 66 : 0.033\n",
      "('I', 'shall') : 64 : 0.032\n",
      "('I', 'know') : 56 : 0.028\n",
      "('I', 'can') : 56 : 0.028\n",
      "('I', 'could') : 56 : 0.028\n",
      "('I', 'think') : 55 : 0.027\n",
      "('I', 'believe') : 47 : 0.023\n",
      "('I', 'hope') : 42 : 0.021\n",
      "('I', 'cannot') : 40 : 0.02\n",
      "('I', 'dare') : 36 : 0.018\n",
      "('I', 'would') : 35 : 0.017\n",
      "('I', 'never') : 35 : 0.017\n",
      "('I', 'must') : 34 : 0.017\n",
      "('I', 'will') : 33 : 0.016\n",
      "('I', 'suppose') : 32 : 0.016\n",
      "('I', 'thought') : 29 : 0.014\n",
      "('I', 'did') : 25 : 0.012\n",
      "('I', 'wish') : 24 : 0.012\n",
      "('I', 'felt') : 18 : 0.009\n",
      "('I', 'assure') : 17 : 0.008\n",
      "('I', 'may') : 16 : 0.008\n",
      "('I', 'saw') : 14 : 0.007\n",
      "('I', 'ever') : 13 : 0.006\n",
      "('I', 'see') : 13 : 0.006\n",
      "('I', ',') : 13 : 0.006\n",
      "('I', 'might') : 12 : 0.006\n",
      "('I', 'wonder') : 11 : 0.005\n",
      "('I', 'understand') : 11 : 0.005\n",
      "('I', 'declare') : 10 : 0.005\n",
      "('I', 'tell') : 9 : 0.004\n",
      "('I', 'heard') : 9 : 0.004\n",
      "('I', 'confess') : 9 : 0.004\n",
      "('I', 'mean') : 8 : 0.004\n",
      "('I', 'remember') : 8 : 0.004\n",
      "('I', 'only') : 7 : 0.003\n",
      "('I', 'knew') : 7 : 0.003\n",
      "('I', 'love') : 7 : 0.003\n",
      "('I', 'certainly') : 7 : 0.003\n",
      "('I', 'always') : 7 : 0.003\n",
      "('I', 'don') : 6 : 0.003\n",
      "('I', 'feel') : 6 : 0.003\n",
      "('I', 'said') : 6 : 0.003\n",
      "('I', \"'\") : 6 : 0.003\n",
      "('I', 'looked') : 6 : 0.003\n",
      "('I', 'hardly') : 6 : 0.003\n",
      "('I', 'told') : 5 : 0.002\n",
      "('I', 'fear') : 5 : 0.002\n",
      "('I', 'to') : 5 : 0.002\n",
      "('I', 'left') : 5 : 0.002\n",
      "('I', 'found') : 5 : 0.002\n",
      "('I', 'beg') : 5 : 0.002\n",
      "('I', 'came') : 5 : 0.002\n",
      "('I', 'like') : 5 : 0.002\n",
      "('I', 'really') : 4 : 0.002\n",
      "('I', 'hear') : 4 : 0.002\n",
      "('I', 'WILL') : 4 : 0.002\n",
      "('I', 'went') : 4 : 0.002\n",
      "('I', 'now') : 4 : 0.002\n",
      "('I', 'come') : 4 : 0.002\n",
      "('I', 'fancy') : 3 : 0.001\n",
      "('I', 'happened') : 3 : 0.001\n",
      "('I', 'talked') : 3 : 0.001\n",
      "('I', 'sent') : 3 : 0.001\n",
      "('I', 'wanted') : 3 : 0.001\n",
      "('I', 'ought') : 3 : 0.001\n",
      "('I', 'ask') : 3 : 0.001\n",
      "('I', 'say') : 3 : 0.001\n",
      "('I', 'made') : 3 : 0.001\n",
      "('I', 'owed') : 3 : 0.001\n",
      "('I', 'DID') : 3 : 0.001\n",
      "('I', 'consider') : 3 : 0.001\n",
      "('I', 'owe') : 3 : 0.001\n",
      "('I', 'advise') : 3 : 0.001\n",
      "('I', 'loved') : 3 : 0.001\n",
      "('I', 'meant') : 3 : 0.001\n",
      "('I', 'been') : 3 : 0.001\n",
      "('I', 'spent') : 3 : 0.001\n",
      "('I', 'well') : 3 : 0.001\n",
      "('I', 'often') : 3 : 0.001\n",
      "('I', 'warrant') : 3 : 0.001\n",
      "('I', 'then') : 3 : 0.001\n",
      "('I', 'walked') : 2 : 0.001\n",
      "('I', 'call') : 2 : 0.001\n",
      "('I', 'got') : 2 : 0.001\n",
      "('I', 'speak') : 2 : 0.001\n",
      "('I', 'returned') : 2 : 0.001\n",
      "('I', 'guessed') : 2 : 0.001\n",
      "('I', 'suffered') : 2 : 0.001\n",
      "('I', 'learnt') : 2 : 0.001\n",
      "('I', 'value') : 2 : 0.001\n",
      "('I', 'admire') : 2 : 0.001\n",
      "('I', 'once') : 2 : 0.001\n",
      "('I', 'entreat') : 2 : 0.001\n",
      "('I', 'need') : 2 : 0.001\n",
      "('I', 'first') : 2 : 0.001\n",
      "('I', 'compare') : 2 : 0.001\n",
      "('I', 'took') : 2 : 0.001\n",
      "('I', 'understood') : 2 : 0.001\n",
      "('I', 'gave') : 2 : 0.001\n",
      "('I', 'make') : 2 : 0.001\n",
      "('I', 'removed') : 2 : 0.001\n",
      "('I', '--') : 2 : 0.001\n",
      "('I', 'just') : 2 : 0.001\n",
      "('I', 'forget') : 2 : 0.001\n",
      "('I', 'thank') : 2 : 0.001\n",
      "('I', 'suffer') : 2 : 0.001\n",
      "('I', 'go') : 2 : 0.001\n",
      "('I', 'COULD') : 2 : 0.001\n",
      "('I', 'require') : 2 : 0.001\n",
      "('I', 'ran') : 2 : 0.001\n",
      "('I', 'detest') : 2 : 0.001\n",
      "('I', 'who') : 2 : 0.001\n",
      "('I', 'used') : 2 : 0.001\n",
      "('I', 'rather') : 2 : 0.001\n",
      "('I', 'shan') : 2 : 0.001\n",
      "('I', 'even') : 2 : 0.001\n",
      "('I', 'guess') : 2 : 0.001\n",
      "('I', 'care') : 2 : 0.001\n",
      "('I', 'watched') : 2 : 0.001\n",
      "('I', 'wished') : 2 : 0.001\n",
      "('I', 'met') : 2 : 0.001\n",
      "('I', 'were') : 2 : 0.001\n",
      "('I', 'died') : 2 : 0.001\n",
      "('I', 'turn') : 1 : 0.0\n",
      "('I', 'known') : 1 : 0.0\n",
      "('I', 'suspect') : 1 : 0.0\n",
      "('I', 'travelled') : 1 : 0.0\n",
      "('I', 'admired') : 1 : 0.0\n",
      "('I', 'profess') : 1 : 0.0\n",
      "('I', 'mentioned') : 1 : 0.0\n",
      "('I', 'expected') : 1 : 0.0\n",
      "('I', 'abhor') : 1 : 0.0\n",
      "('I', 'take') : 1 : 0.0\n",
      "('I', 'longed') : 1 : 0.0\n",
      "('I', 'DO') : 1 : 0.0\n",
      "('I', 'sha') : 1 : 0.0\n",
      "('I', 'write') : 1 : 0.0\n",
      "('I', 'more') : 1 : 0.0\n",
      "('I', 'pity') : 1 : 0.0\n",
      "('I', 'stop') : 1 : 0.0\n",
      "('I', 'heartily') : 1 : 0.0\n",
      "('I', 'give') : 1 : 0.0\n",
      "('I', 'directly') : 1 : 0.0\n",
      "('I', 'both') : 1 : 0.0\n",
      "('I', 'imagine') : 1 : 0.0\n",
      "('I', 'trusted') : 1 : 0.0\n",
      "('I', 'question') : 1 : 0.0\n",
      "('I', 'alluded') : 1 : 0.0\n",
      "('I', 'acquit') : 1 : 0.0\n",
      "('I', 'endured') : 1 : 0.0\n",
      "('I', 'allowed') : 1 : 0.0\n",
      "('I', 'earnestly') : 1 : 0.0\n",
      "('I', 'remain') : 1 : 0.0\n",
      "('I', 'avoided') : 1 : 0.0\n",
      "('I', 'insist') : 1 : 0.0\n",
      "('I', 'stay') : 1 : 0.0\n",
      "('I', 'quitted') : 1 : 0.0\n",
      "('I', 'formed') : 1 : 0.0\n",
      "('I', 'copied') : 1 : 0.0\n",
      "('I', 'dreaded') : 1 : 0.0\n",
      "('I', 'spoken') : 1 : 0.0\n",
      "('I', 'not') : 1 : 0.0\n",
      "('I', 'distress') : 1 : 0.0\n",
      "('I', 'immediately') : 1 : 0.0\n",
      "('I', 'tried') : 1 : 0.0\n",
      "('I', 'protest') : 1 : 0.0\n",
      "('I', 'find') : 1 : 0.0\n",
      "('I', 'sat') : 1 : 0.0\n",
      "('I', 'feared') : 1 : 0.0\n",
      "('I', 'express') : 1 : 0.0\n",
      "('I', 'clearly') : 1 : 0.0\n",
      "('I', 'venture') : 1 : 0.0\n",
      "('I', 'trust') : 1 : 0.0\n",
      "('I', 'begged') : 1 : 0.0\n",
      "('I', 'fell') : 1 : 0.0\n",
      "('I', 'look') : 1 : 0.0\n",
      "('I', 'professed') : 1 : 0.0\n",
      "('I', 'no') : 1 : 0.0\n",
      "('I', 'flatter') : 1 : 0.0\n",
      "('I', 'acknowledge') : 1 : 0.0\n",
      "('I', 'observed') : 1 : 0.0\n",
      "('I', 'conceal') : 1 : 0.0\n",
      "('I', 'offered') : 1 : 0.0\n",
      "('I', 'want') : 1 : 0.0\n",
      "('I', 'contradicted') : 1 : 0.0\n",
      "('I', 'advised') : 1 : 0.0\n",
      "('I', 'undervalued') : 1 : 0.0\n",
      "('I', 'TRIED') : 1 : 0.0\n",
      "('I', '.') : 1 : 0.0\n",
      "('I', 'demand') : 1 : 0.0\n",
      "('I', 'AM') : 1 : 0.0\n",
      "('I', 'supply') : 1 : 0.0\n",
      "('I', 'remained') : 1 : 0.0\n",
      "('I', 'less') : 1 : 0.0\n",
      "('I', 'still') : 1 : 0.0\n",
      "('I', 'blundered') : 1 : 0.0\n",
      "('I', 'neither') : 1 : 0.0\n",
      "('I', 'scorn') : 1 : 0.0\n",
      "('I', 'bring') : 1 : 0.0\n",
      "('I', 'endeavoured') : 1 : 0.0\n",
      "('I', 'doubt') : 1 : 0.0\n",
      "('I', 'discharged') : 1 : 0.0\n",
      "('I', 'imitate') : 1 : 0.0\n",
      "('I', 'cut') : 1 : 0.0\n",
      "('I', 'obey') : 1 : 0.0\n",
      "('I', 'greatly') : 1 : 0.0\n",
      "('I', 'particularly') : 1 : 0.0\n",
      "('I', 'meet') : 1 : 0.0\n",
      "('I', 'wrote') : 1 : 0.0\n",
      "('I', 'SHOULD') : 1 : 0.0\n",
      "('I', 'reconciled') : 1 : 0.0\n",
      "('I', 'a') : 1 : 0.0\n",
      "('I', 'began') : 1 : 0.0\n",
      "('I', 'reflect') : 1 : 0.0\n",
      "('I', 'approached') : 1 : 0.0\n",
      "('I', 'won') : 1 : 0.0\n",
      "('I', '.--') : 1 : 0.0\n",
      "('I', 'reserved') : 1 : 0.0\n",
      "('I', ',\"') : 1 : 0.0\n",
      "('I', 'leave') : 1 : 0.0\n",
      "('I', 'set') : 1 : 0.0\n",
      "('I', 'WAS') : 1 : 0.0\n",
      "('I', 'rich') : 1 : 0.0\n",
      "('I', 'quite') : 1 : 0.0\n",
      "('I', 'KNEW') : 1 : 0.0\n",
      "('I', 'long') : 1 : 0.0\n",
      "('I', 'chose') : 1 : 0.0\n",
      "('I', 'convinced') : 1 : 0.0\n",
      "('I', 'called') : 1 : 0.0\n",
      "('I', 'received') : 1 : 0.0\n",
      "('I', 'stayed') : 1 : 0.0\n",
      "('I', 'considered') : 1 : 0.0\n",
      "('I', 'visited') : 1 : 0.0\n",
      "('I', 'seem') : 1 : 0.0\n",
      "('I', 'explained') : 1 : 0.0\n",
      "('I', 'next') : 1 : 0.0\n",
      "('I', 'cease') : 1 : 0.0\n",
      "('I', 'delighted') : 1 : 0.0\n",
      "('I', 'dared') : 1 : 0.0\n",
      "('I', 'resolved') : 1 : 0.0\n",
      "('I', 'presume') : 1 : 0.0\n",
      "('I', 'sincerely') : 1 : 0.0\n",
      "('I', 'persuaded') : 1 : 0.0\n",
      "('I', 'wear') : 1 : 0.0\n",
      "('I', 'meditated') : 1 : 0.0\n",
      "('I', 'formerly') : 1 : 0.0\n",
      "('I', 'relate') : 1 : 0.0\n",
      "('I', 'an') : 1 : 0.0\n"
     ]
    }
   ],
   "source": [
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" :\n",
    "        print(\" : \".join([str(gram),str(count),str(round(count/total_words,3))]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "223/192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 1598),\n",
       " ((\"'\", 's'), 700),\n",
       " ((';', 'and'), 605),\n",
       " (('Mrs', '.'), 529),\n",
       " (('of', 'the'), 430),\n",
       " (('.\"', '\"'), 428),\n",
       " (('to', 'be'), 428),\n",
       " ((',', '\"'), 392),\n",
       " (('.', '\"'), 369),\n",
       " (('in', 'the'), 348)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.ngrams(text2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'am', 'sure') : 72\n",
      "('I', 'am', 'not') : 12\n",
      "('I', 'am', 'so') : 11\n",
      "('I', 'am', 'sorry') : 11\n",
      "('I', 'am', 'afraid') : 11\n",
      "('I', 'am', 'very') : 10\n",
      "('I', 'am', 'glad') : 4\n",
      "('I', 'am', 'now') : 4\n",
      "('I', 'am', 'monstrous') : 4\n",
      "('I', 'am', ',') : 3\n",
      "('I', 'am', 'much') : 3\n",
      "('I', 'am', 'convinced') : 3\n",
      "('I', 'am', 'always') : 3\n",
      "('I', 'am', 'in') : 3\n",
      "('I', 'am', 'perfectly') : 3\n",
      "('I', 'am', 'alive') : 2\n",
      "('I', 'am', 'only') : 2\n",
      "('I', 'am', 'almost') : 2\n",
      "('I', 'am', 'well') : 2\n",
      "('I', 'am', 'the') : 2\n",
      "('I', 'am', 'extremely') : 2\n",
      "('I', 'am', 'to') : 2\n",
      "('I', 'am', 'quite') : 2\n",
      "('I', 'am', 'particularly') : 2\n",
      "('I', 'am', 'capable') : 2\n",
      "('I', 'am', 'grown') : 2\n",
      "('I', 'am', 'unable') : 1\n",
      "('I', 'am', 'resolved') : 1\n",
      "('I', 'am', '.') : 1\n",
      "('I', 'am', 'doing') : 1\n",
      "('I', 'am', 'disappointed') : 1\n",
      "('I', 'am', 'writing') : 1\n",
      "('I', 'am', ';') : 1\n",
      "('I', 'am', 'happy') : 1\n",
      "('I', 'am', 'allowed') : 1\n",
      "('I', 'am', 'right') : 1\n",
      "('I', 'am', 'wretched') : 1\n",
      "('I', 'am', 'miserable') : 1\n",
      "('I', 'am', 'shut') : 1\n",
      "('I', 'am', 'flattered') : 1\n",
      "('I', 'am', 'thankful') : 1\n",
      "('I', 'am', 'guilty') : 1\n",
      "('I', 'am', 'by') : 1\n",
      "('I', 'am', 'at') : 1\n",
      "('I', 'am', 'rather') : 1\n",
      "('I', 'am', 'with') : 1\n",
      "('I', 'am', 'ready') : 1\n",
      "('I', 'am', 'ever') : 1\n",
      "('I', 'am', 'talking') : 1\n",
      "('I', 'am', 'NOT') : 1\n",
      "('I', 'am', 'charged') : 1\n",
      "('I', 'am', 'persuaded') : 1\n",
      "('I', 'am', ',\"') : 1\n",
      "('I', 'am', 'delighted') : 1\n",
      "('I', 'am', 'heartily') : 1\n",
      "('I', 'am', 'bound') : 1\n",
      "('I', 'am', 'amazingly') : 1\n",
      "('I', 'am', 'no') : 1\n",
      "('I', 'am', 'ruined') : 1\n",
      "('I', 'am', 'informed') : 1\n",
      "('I', 'am', 'able') : 1\n",
      "('I', 'am', 'determined') : 1\n",
      "('I', 'am', 'before') : 1\n",
      "('I', 'am', 'justified') : 1\n",
      "('I', 'am', 'a') : 1\n",
      "('I', 'am', 'excessively') : 1\n",
      "('I', 'am', 'going') : 1\n",
      "('I', 'am', 'commissioned') : 1\n",
      "('I', 'am', 'come') : 1\n",
      "('I', 'am', 'confined') : 1\n",
      "0.32286995515695066\n",
      "0.053811659192825115\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "\n",
    "for gram,count in sorted(fd.items(), key=lambda pair: pair[1], reverse=True) : \n",
    "    if gram[0] == \"I\" and gram[1] == \"am\" :\n",
    "        total_words += count\n",
    "        print(\" : \".join([str(gram),str(count)])) \n",
    "        \n",
    "\n",
    "print(72/total_words)\n",
    "print(12/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2004 matches:\n",
      " to me ,\" replied her husband , \" that I should assist his widow and daughters \n",
      " did not know what he was talking of , I dare say ; ten to one but he was light\n",
      "ly to myself . He could hardly suppose I should neglect them . But as he requir\n",
      "hem . But as he required the promise , I could not do less than give it ; at le\n",
      "ld not do less than give it ; at least I thought so at the time . The promise ,\n",
      "t you have such a generous spirit !\" \" I would not wish to do any thing mean ,\"\n",
      "little . No one , at least , can think I have not done enough for them : even t\n",
      "can afford to do .\" \" Certainly -- and I think I may afford to give them five h\n",
      "rd to do .\" \" Certainly -- and I think I may afford to give them five hundred p\n",
      " That is very true , and , therefore , I do not know whether , upon the whole ,\n",
      " them -- something of the annuity kind I mean .-- My sisters would feel the goo\n",
      " are not aware of what you are doing . I have known a great deal of the trouble\n",
      "such an abhorrence of annuities , that I am sure I would not pin myself down to\n",
      "horrence of annuities , that I am sure I would not pin myself down to the payme\n",
      "and it raises no gratitude at all . If I were you , whatever I did should be do\n",
      "tude at all . If I were you , whatever I did should be done at my own discretio\n",
      "e done at my own discretion entirely . I would not bind myself to allow them an\n",
      "ifty pounds from our own expenses .\" \" I believe you are right , my love ; it w\n",
      "d by no annuity in the case ; whatever I may give them occasionally will be of \n",
      "eing distressed for money , and will , I think , be amply discharging my promis\n",
      " it will . Indeed , to say the truth , I am convinced within myself that your f\n",
      "t all . The assistance he thought of , I dare say , was only such as might be r\n",
      " forth , whenever they are in season . I ' ll lay my life that he meant nothing\n",
      "e they will be ! Five hundred a year ! I am sure I cannot imagine how they will\n",
      "l be ! Five hundred a year ! I am sure I cannot imagine how they will spend hal\n"
     ]
    }
   ],
   "source": [
    "text2.concordance(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need this for phrases\n",
    "from nltk.app import concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 136 matches:\n",
      "ur poor little boy --\" \" Why , to be sure ,\" said her husband , very gravely ,\n",
      " very convenient addition .\" \" To be sure it would .\" \" Perhaps , then , it wo\n",
      "rtune for any young woman .\" \" To be sure it is ; and , indeed , it strikes me\n",
      " them . If they marry , they will be sure of doing well , and if they do not ,\n",
      "g her consent to this plan . \" To be sure ,\" said she , \" it is better than pa\n",
      " abhorrence of annuities , that I am sure I would not pin myself down to the p\n",
      "e their style of living if they felt sure of a larger income , and would not b\n",
      "g my promise to my father .\" \" To be sure it will . Indeed , to say the truth \n",
      "will be ! Five hundred a year ! I am sure I cannot imagine how they will spend\n",
      "and if THAT were your opinion , I am sure you could never be civil to him .\" M\n",
      "that is worthy and amiable .\" \" I am sure ,\" replied Elinor , with a smile , \"\n",
      "n travelling so far to see me , I am sure I will find none in accommodating th\n",
      " . \" As for the house itself , to be sure ,\" said she , \" it is too small for \n",
      "ich I cannot conceal from you . I am sure Edward Ferrars is not well . We have\n",
      " Mrs . Dashwood . \" Know him ! to be sure I do . Why , he is down here every y\n",
      "se would be a trifle ; Mamma she was sure would never object to it ; and any h\n",
      "et to tell you about Marianne . I am sure she will be married to Mr . Willough\n",
      "d this is quite another thing . I am sure they will be married very soon , for\n",
      "r , it is Marianne ' s . I am almost sure it is , for I saw him cut it off . L\n",
      " ; at his own house at Norland to be sure . He is the curate of the parish I d\n",
      "is lately dead , Marianne , for I am sure there was such a man once , and his \n",
      "s ; it is about Miss Williams , I am sure .\" \" And who is Miss Williams ?\" ask\n",
      "you know who Miss Williams is ? I am sure you must have heard of her before . \n",
      "what could be the reason of it ; was sure there must be some bad news , and th\n",
      "melancholy must be the matter , I am sure ,\" said she . \" I could see it in hi\n"
     ]
    }
   ],
   "source": [
    "text2.concordance(\"sure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(freq_dist):\n",
    "    weight_total = sum([count for token,count in freq_dist.items()])\n",
    "    n = random.uniform(0, weight_total)\n",
    "    for token, count in freq_dist.items() :\n",
    "        if n < count:\n",
    "            return(token)\n",
    "        n = n - count\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_choice(FreqDist(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', Moby respect had three fights when he their you'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your seeing dislike liberty would I know Jennings ; .'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'puff here m is is , .. probably i that'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_choice_ngram(cur_word,freq_dist) :\n",
    "    ''' Starts with a current word and randomly chooses \n",
    "        a following word based on the bigrams. '''\n",
    "    \n",
    "    # First, build list of tuples of the form\n",
    "    # ('a_word',count)\n",
    "    # where our freq_dist has an entry like \n",
    "    # ('cur_word','a_word',count)\n",
    "    sub_dist = {}\n",
    "    \n",
    "    for bigram, count in freq_dist.items() :\n",
    "        if bigram[0] == cur_word :\n",
    "            sub_dist[bigram[1]] = count\n",
    "    \n",
    "    return(weighted_choice(sub_dist))\n",
    "\n",
    "def generate_bigram(text,length=10,start=None) :\n",
    "    \n",
    "    if not start :\n",
    "        uni_fd = FreqDist(text)\n",
    "        start = weighted_choice(uni_fd)\n",
    "        \n",
    "    fd = FreqDist(nltk.bigrams(text))\n",
    "    \n",
    "    results = []\n",
    "    this_word = start\n",
    "    for i in range(length) :\n",
    "        this_word = weighted_choice_ngram(this_word,fd)\n",
    "        results.append(this_word)\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the lone whale are infernal aforethought of angels mobbing thee'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', his distress beyond a sore throat .-- When they'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gas is U37 PART ! U156 No ? :) ok'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
