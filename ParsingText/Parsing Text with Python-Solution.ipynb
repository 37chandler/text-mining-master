{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Text with Python-Solution\n",
    "This notebook will work us through some of the basic examples of parsing text with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "In NLP _concordance_ are words that co-occur with a word of interest. Let's look at some examples. Concordances let us see words in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How do Austen and Melville seem to be using \"monstrous\" differently?\n",
    "\n",
    "Play around with some other words and other corpora. (You knew that was the plural, right? Yeah, me too.) Here's one to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text5.concordance(':P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Similarity\n",
    "While concordance let's us see context. _Similarity_ lets us see what words share that kind of context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, play around with some different words and different corpora. Another one to get you started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text5.similar(\"lol\") # The rise of 'haha' began as early as the mid-aughts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Contexts\n",
    "Common contexts take this to the next level, as you can now define a list of words and see the contexts they tend to have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.common_contexts([\"monstrous\",\"curious\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\",\"very\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underscore is giving you the spot where either of these words are likely to appear. Pretty cool. \n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing Word Usage\n",
    "As you know, I love me some data viz. NLTK gives you a way to visualize the distribution of words in a corpus called a dispersion plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [\"citizens\",\"democracy\",\"freedom\",\"America\",\"duties\",\"fear\"]\n",
    "\n",
    "text4.dispersion_plot(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vertical line represents a word usage. The `offset` is how deep into the corpus it is. Usually that's not so helpful, but for the inaugural corpus (or any corpus that's ordered chronologically), it gives us a time-series view. This corpus runs through Obama's 2009 address. Feel free to play around with some of the other corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in the chat there's much less of that times-series idea.\n",
    "text5.dispersion_plot([\":-)\",\"lol\",\"haha\",\";-)\",\"hey\",\"hi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics on Text\n",
    "Now we'll dive into the basics of summary statistics on text. I'll add comments in the code so you can tell what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\" : \".join([text1.name,str(len(text1))]))\n",
    "print(\" : \".join([text2.name,str(len(text2))])) \n",
    "# the length in \"tokens\". We'll talk more about those, \n",
    "# but think of them as characters we want to group together like \"Ishmael\", \"Dr. Snodhead\" and \":-)\"\n",
    "\n",
    "print(sorted(set(text3))[1:20]) # the first 20 tokens in Genesis\n",
    "print(len(set(text1))) # the number of tokens in Moby Dick\n",
    "\n",
    "# If we wanted to see the average number of times a token is used, across all tokens, we can do this: \n",
    "print(len(text1)/len(set(text1))) # this is called lexical diversity\n",
    "\n",
    "# If we wanted to see how often a word was used, we just do\n",
    "print(text1.count(\"whale\"))\n",
    "\n",
    "# and, as a percentage\n",
    "print(100 * text1.count(\"the\")/len(text1)) # 5% of the words in Moby Dick are \"the\". Writing is easy! ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore some of these summary stats on your own. Find me an interesting usage in the chat data or something. Keep it clean...\n",
    "\n",
    "Let's write some functions that calculate these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary Statistic Functions\n",
    "def lexical_diversity(text) :\n",
    "    # Write a function that returns the lexical diversity of a text as calculated above.\n",
    "    return(len(text)/len(set(text)))\n",
    "    \n",
    "def token_percentage(text, word) :\n",
    "    # write a function that takes a text and a word and returns the percentage\n",
    "    # of words in `text` that are `word`\n",
    "    return(100 * text.count(\"the\")/len(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lexical_diversity(text4))\n",
    "assert(14.9 < lexical_diversity(text4) < 15) # testing our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(token_percentage(text5,\"lol\"))\n",
    "assert(1.43 < token_percentage(text5,\"lol\") < 1.44) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In NLTK, texts are basically just lists of words. You can see that behavior by writing something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 143000 # just picking something that lands us in Obama's address\n",
    "print(text4[x:(x+20)])\n",
    "print(\" \".join(text4[x:(x+200)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all of our list tricks like slices and whatnot will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Frequency Distributions\n",
    "One of the most common ways places to start understanding a corpus is to do a frequency analysis. NLTK makes that pretty easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moby_fd = FreqDist(text1) # This returns a Counter object, which we learned about a bit in the fall. \n",
    "    # basically a dictionary that's optimized for counting. Let's look at the top 50\n",
    "    \n",
    "moby_fd.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about these keys? How many of them are informative about the text as you skim that list?\n",
    "\n",
    "To compare, let's look at S&S:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss_fd = FreqDist(text2)\n",
    "ss_fd.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these words are what we call \"stopwords\". These are words that commonly occur in most writing. More on these below.\n",
    "\n",
    "Let's plot the cumulative distribution of words in _Moby Dick_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moby_fd.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this text has 260K words. So the top 50 account for almost 50%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Finding Specific Words\n",
    "Thus far we've worked with a corpus in a general way. Now let's find some words that are interesting to us specifically. We'll take advantage of this \"list of words\" idea. First, let's look at the long words that are used by presidents and compare them to long words used in chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Presidential addresses\n",
    "print(sorted({w.lower() for w in set(text4) if len(w) > 15}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Versus chat rooms. Sigh. Is this the beginning of the end of written discourse? \n",
    "print(sorted({w.lower() for w in set(text5) if len(w) > 15}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these are probably only used once. Let's get longer words that are used a few times in chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat_dist = FreqDist(text5)\n",
    "\n",
    "print(sorted({w.lower() for w in set(text5) if len(w) > 7 and chat_dist[w] > 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not great, but now we're starting to get some signal in here about what people are chatting about.\n",
    "\n",
    "---\n",
    "\n",
    "## Bigrams\n",
    "Bigrams are pairs of words in a text. For instance, here are the bigrams in that previous sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "list(bigrams([\"bigrams\",\"are\",\"pairs\",\"of\",\"words\",\"in\",\"a\",\"text\"])) # wrap in list to get printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the popular bigrams. NLTK gives us the function `collocations` that make this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text8.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the idea of counting words to count other things. In the cell below we get the frequency distribution of lengths of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_len_count = FreqDist([len(w) for w in text1])\n",
    "\n",
    "word_len_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_len_count.freq(1) # `.freq` gives us the percentages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Other Corpora\n",
    "There are a lot of corpora available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.shakespeare.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lexical Resources\n",
    "There are corpora that we may use to generate further analyses. There's one that's just English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's a corpus with 236,736 english words\n",
    "len(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = nltk.corpus.words.words()\n",
    "\n",
    "print(x[100200:100210])\n",
    "print(\"mark\" in x) # you can use this as a rudimentary spell checker\n",
    "print(\"makr\" in x) # but more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Lewis recently published a new book, _The Undoing Project_. It's great and you should totally read it. It's about two researchers who uncovered and tested many of the fundamental biases and heuristics. One of these is the [availability heuristic](https://en.wikipedia.org/wiki/Availability_heuristic). One example they give is the following: do you think there are more words that start with \"k\" or have \"k\" in the third letter? It's much easier for people to think of words that start with a letter than have letters in other spots. \n",
    "\n",
    "We can test that here. Let's count the words with letters in each spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([w for w in nltk.corpus.words.words() if len(w) >= 3 and w[0] == \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([w for w in set(text4) if len(w) >= 3 and w[0] == \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[w for w in set(text1) if len(w) >= 3 and w[0] == \"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([w for w in nltk.corpus.words.words() if len(w) >= 3 and w[2] == \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([w for w in set(text4) if len(w) >= 3 and w[2] == \"k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't appear to be much evidence for it here. Look at some of the words in the first list and give me a hypothesis as to what's going on. Test your hypothesis using one of these corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find words in S&S that are unusual. We'll start with a function\n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return(sorted(unusual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt')) # You'll see a lot of verb tenses and the like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are also stopwords. These are very common words.\n",
    "list(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this might be useful to get a freqdist that's not dominated by stopwords and punctuation with the words in lowercase.\n",
    "ss_fd = FreqDist([w.lower() for w in nltk.corpus.gutenberg.words('austen-sense.txt') \n",
    "                  if (w.lower() not in nltk.corpus.stopwords.words(\"english\") \n",
    "                      and w.isalpha())])\n",
    "\n",
    "# This takes a bit of time to run. There are faster ways to do this using sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_fd.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_fd = FreqDist([w.lower() for w in nltk.corpus.inaugural.words() \n",
    "                  if (w.lower() not in nltk.corpus.stopwords.words(\"english\") \n",
    "                      and w.isalpha())])\n",
    "in_fd.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a giant corpus of names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids()\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [w for w in male_names if w in female_names]\n",
    "x[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [w for w in male_names if w not in female_names]\n",
    "x[1:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's semi-well known that that names ending in \"a\" are almost always female. Let's visualize names by last letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = nltk.ConditionalFreqDist(\n",
    "    (fileid, name[-1])\n",
    "    for fileid in names.fileids()\n",
    "    for name in names.words(fileid))\n",
    "\n",
    "# This uses some trickery from a function we haven't talked about. \n",
    "# Try to figure out ConditionalFreqDist! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# B is not a popular last letter. Let's look at those\n",
    "[w for w in female_names if w[-1]==\"b\"] # change to female to see those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# WordNet\n",
    "WordNet gives us meanings and synonyms for many words. We're just going to scratch the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('bottle') # there are 5 \"synonym sets\" for bottle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('bottle.v.01').definition() # change the stuff in quotes to see others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out set!\n",
    "wn.synsets('set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synset('plant.v.01').lemma_names() # Linguists call synonyms \"lemmas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at them all...\n",
    "for synset in wn.synsets('set'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk more about lexical relations further down the road...\n",
    "\n",
    "Before we leave this, let's take a look at some other corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type a period after \"corpus\" below, hit tab, and check out all the lowercase options--these \n",
    "# are corpora. Google one to see what's in it.\n",
    "nltk.corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Your Own\n",
    "It's fun to play around with corpora from other people, and we'll use these multiple times. But it's also nice to load your own corpus. Let's load in the Twitter descriptions you pulled from one of your files. First, we'll just read in the data normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change these next two lines based on your data\n",
    "file_location = \"C:\\\\Users\\\\jchan\\\\Dropbox\\\\Teaching\\\\2017_Spring\\\\UnstructuredData\\\\PreWork\\\\\"\n",
    "file_name = \"20170305_GeneralMills_followers.txt\"\n",
    "#file_name = \"20170305_michaelpollan_followers.txt\"\n",
    "\n",
    "descs = []\n",
    "with open(file_location + file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 6 has the description\n",
    "        if len(line) >= 7 : # sometimes we don't have descriptions\n",
    "            descs.extend(line[6].split())\n",
    "        \n",
    "        # for now we'll just add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(file_location + file_name,'r') as ifile :\n",
    "    print(ifile.readline())\n",
    "    print(ifile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many descriptions do we have?\n",
    "len(descs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some of our techniques on this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(descs)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get serious about cleaning a list like we did above by writing a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_sp = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "\n",
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to lowercase\n",
    "        * stopwords removed\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.lower() for w in text if w.isalpha()]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords_sp]\n",
    "    return(text_clean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descs_clean_gm = clean_list(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_mp = FreqDist(descs_clean_mp)\n",
    "fd_mp.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_gm = FreqDist(descs_clean_gm)\n",
    "fd_gm.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go do the same for your other file. I'll ask you to report back on the similarities and differences in a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
